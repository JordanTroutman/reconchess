{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('datafile.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RANK</th>\n",
       "      <th>FILE</th>\n",
       "      <th>SCORE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>MOVES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>47278.000000</td>\n",
       "      <td>47278.000000</td>\n",
       "      <td>47278.000000</td>\n",
       "      <td>47278.000000</td>\n",
       "      <td>47278.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.840306</td>\n",
       "      <td>3.541309</td>\n",
       "      <td>-2.606667</td>\n",
       "      <td>754.948271</td>\n",
       "      <td>47.939549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.143153</td>\n",
       "      <td>2.248801</td>\n",
       "      <td>11.123017</td>\n",
       "      <td>188.877364</td>\n",
       "      <td>9.602971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-37.000000</td>\n",
       "      <td>-0.036504</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-7.000000</td>\n",
       "      <td>709.242565</td>\n",
       "      <td>44.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>838.929584</td>\n",
       "      <td>49.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>879.465136</td>\n",
       "      <td>59.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>899.999661</td>\n",
       "      <td>83.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               RANK          FILE         SCORE          TIME         MOVES\n",
       "count  47278.000000  47278.000000  47278.000000  47278.000000  47278.000000\n",
       "mean       3.840306      3.541309     -2.606667    754.948271     47.939549\n",
       "std        2.143153      2.248801     11.123017    188.877364      9.602971\n",
       "min        0.000000      0.000000    -37.000000     -0.036504     20.000000\n",
       "25%        2.000000      2.000000     -7.000000    709.242565     44.000000\n",
       "50%        4.000000      4.000000     -1.000000    838.929584     49.000000\n",
       "75%        6.000000      5.000000      2.000000    879.465136     59.000000\n",
       "max        7.000000      7.000000     37.000000    899.999661     83.000000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -37 to -13 Low\n",
    "# -12 to 12  Medium\n",
    "# 13  to 37  High\n",
    "\n",
    "categories = [data[\"SCORE\"].between(-37, -13), data[\"SCORE\"].between(-12, 12), data['SCORE'].between(13, 37)]\n",
    "values = [\"LOW\", \"MEDIUM\", \"HIGH\"]\n",
    "data[\"SCORE\"] = np.select(categories, values, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop([\"SCORE\"], axis=1)\n",
    "Y = data[\"SCORE\"].map({\"HIGH\":1, \"MEDIUM\":1, \"LOW\": 0})\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic Regression does not work too well.  I mean it has a 86% success rate which could be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jordantroutman/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression().fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8667478528393796"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "We'll see if NN will work any better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from math import floor, ceil\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(series): \n",
    "    return pd.get_dummies(series.astype(str))\n",
    "X = data.drop([\"SCORE\"], axis=1)\n",
    "Y = encode(data.SCORE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jordantroutman/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "def multilayer_perceptron(x, weights, biases, keep_prob):\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_1 = tf.nn.dropout(layer_1, keep_prob)\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "\n",
    "n_hidden_1 = 10\n",
    "n_input = train_x.shape[1]\n",
    "n_classes = train_y.shape[1]\n",
    "\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_1, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_epochs = 2000\n",
    "display_step = 10\n",
    "batch_size = 300\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = multilayer_perceptron(x, weights, biases, keep_prob)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predictions, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 2162.639341969\n",
      "Epoch: 0011 cost= 400.068775312\n",
      "Epoch: 0021 cost= 136.788903970\n",
      "Epoch: 0031 cost= 42.472406253\n",
      "Epoch: 0041 cost= 2.776858679\n",
      "Epoch: 0051 cost= 0.630116126\n",
      "Epoch: 0061 cost= 0.627513955\n",
      "Epoch: 0071 cost= 0.625356512\n",
      "Epoch: 0081 cost= 0.623504923\n",
      "Epoch: 0091 cost= 0.621772409\n",
      "Epoch: 0101 cost= 0.619652261\n",
      "Epoch: 0111 cost= 0.619107620\n",
      "Epoch: 0121 cost= 0.607533794\n",
      "Epoch: 0131 cost= 0.597433243\n",
      "Epoch: 0141 cost= 0.592043585\n",
      "Epoch: 0151 cost= 0.589367825\n",
      "Epoch: 0161 cost= 0.589923530\n",
      "Epoch: 0171 cost= 0.589479073\n",
      "Epoch: 0181 cost= 0.587227967\n",
      "Epoch: 0191 cost= 0.587640151\n",
      "Epoch: 0201 cost= 0.586071797\n",
      "Epoch: 0211 cost= 0.584592795\n",
      "Epoch: 0221 cost= 0.585368422\n",
      "Epoch: 0231 cost= 0.583860227\n",
      "Epoch: 0241 cost= 0.582502192\n",
      "Epoch: 0251 cost= 0.578363417\n",
      "Epoch: 0261 cost= 0.571404827\n",
      "Epoch: 0271 cost= 0.567534147\n",
      "Epoch: 0281 cost= 0.556569130\n",
      "Epoch: 0291 cost= 0.550654686\n",
      "Epoch: 0301 cost= 0.542371515\n",
      "Epoch: 0311 cost= 0.539735193\n",
      "Epoch: 0321 cost= 0.536647181\n",
      "Epoch: 0331 cost= 0.532913300\n",
      "Epoch: 0341 cost= 0.534311431\n",
      "Epoch: 0351 cost= 0.532932005\n",
      "Epoch: 0361 cost= 0.531954177\n",
      "Epoch: 0371 cost= 0.531474545\n",
      "Epoch: 0381 cost= 0.533052505\n",
      "Epoch: 0391 cost= 0.529059627\n",
      "Epoch: 0401 cost= 0.530432552\n",
      "Epoch: 0411 cost= 0.528128807\n",
      "Epoch: 0421 cost= 0.528536440\n",
      "Epoch: 0431 cost= 0.529975189\n",
      "Epoch: 0441 cost= 0.529512718\n",
      "Epoch: 0451 cost= 0.527999013\n",
      "Epoch: 0461 cost= 0.530025835\n",
      "Epoch: 0471 cost= 0.527822839\n",
      "Epoch: 0481 cost= 0.528549857\n",
      "Epoch: 0491 cost= 0.528988188\n",
      "Epoch: 0501 cost= 0.527826679\n",
      "Epoch: 0511 cost= 0.528870251\n",
      "Epoch: 0521 cost= 0.529821820\n",
      "Epoch: 0531 cost= 0.528071604\n",
      "Epoch: 0541 cost= 0.529892224\n",
      "Epoch: 0551 cost= 0.529443103\n",
      "Epoch: 0561 cost= 0.528201061\n",
      "Epoch: 0571 cost= 0.530962271\n",
      "Epoch: 0581 cost= 0.527971294\n",
      "Epoch: 0591 cost= 0.526343695\n",
      "Epoch: 0601 cost= 0.527317655\n",
      "Epoch: 0611 cost= 0.524528849\n",
      "Epoch: 0621 cost= 0.531078838\n",
      "Epoch: 0631 cost= 0.527656567\n",
      "Epoch: 0641 cost= 0.527013198\n",
      "Epoch: 0651 cost= 0.528843174\n",
      "Epoch: 0661 cost= 0.527538325\n",
      "Epoch: 0671 cost= 0.529652118\n",
      "Epoch: 0681 cost= 0.527531178\n",
      "Epoch: 0691 cost= 0.525686092\n",
      "Epoch: 0701 cost= 0.525367902\n",
      "Epoch: 0711 cost= 0.527197845\n",
      "Epoch: 0721 cost= 0.526917208\n",
      "Epoch: 0731 cost= 0.525223932\n",
      "Epoch: 0741 cost= 0.528848528\n",
      "Epoch: 0751 cost= 0.525134361\n",
      "Epoch: 0761 cost= 0.526886314\n",
      "Epoch: 0771 cost= 0.524405350\n",
      "Epoch: 0781 cost= 0.527718582\n",
      "Epoch: 0791 cost= 0.526009573\n",
      "Epoch: 0801 cost= 0.524522355\n",
      "Epoch: 0811 cost= 0.526319593\n",
      "Epoch: 0821 cost= 0.528012455\n",
      "Epoch: 0831 cost= 0.526832379\n",
      "Epoch: 0841 cost= 0.523558367\n",
      "Epoch: 0851 cost= 0.526172689\n",
      "Epoch: 0861 cost= 0.528142173\n",
      "Epoch: 0871 cost= 0.526543193\n",
      "Epoch: 0881 cost= 0.521100906\n",
      "Epoch: 0891 cost= 0.526169612\n",
      "Epoch: 0901 cost= 0.523672967\n",
      "Epoch: 0911 cost= 0.524610111\n",
      "Epoch: 0921 cost= 0.525611290\n",
      "Epoch: 0931 cost= 0.525790447\n",
      "Epoch: 0941 cost= 0.523690293\n",
      "Epoch: 0951 cost= 0.524104799\n",
      "Epoch: 0961 cost= 0.522729892\n",
      "Epoch: 0971 cost= 0.523049423\n",
      "Epoch: 0981 cost= 0.522318047\n",
      "Epoch: 0991 cost= 0.520597533\n",
      "Epoch: 1001 cost= 0.521175060\n",
      "Epoch: 1011 cost= 0.519642328\n",
      "Epoch: 1021 cost= 0.520538667\n",
      "Epoch: 1031 cost= 0.515839649\n",
      "Epoch: 1041 cost= 0.510869615\n",
      "Epoch: 1051 cost= 0.510693566\n",
      "Epoch: 1061 cost= 0.505442309\n",
      "Epoch: 1071 cost= 0.504287786\n",
      "Epoch: 1081 cost= 0.500183429\n",
      "Epoch: 1091 cost= 0.499654537\n",
      "Epoch: 1101 cost= 0.498882511\n",
      "Epoch: 1111 cost= 0.498013926\n",
      "Epoch: 1121 cost= 0.498053036\n",
      "Epoch: 1131 cost= 0.498586988\n",
      "Epoch: 1141 cost= 0.496409671\n",
      "Epoch: 1151 cost= 0.496892868\n",
      "Epoch: 1161 cost= 0.498540627\n",
      "Epoch: 1171 cost= 0.496390386\n",
      "Epoch: 1181 cost= 0.496655951\n",
      "Epoch: 1191 cost= 0.495461881\n",
      "Epoch: 1201 cost= 0.486637433\n",
      "Epoch: 1211 cost= 0.484606566\n",
      "Epoch: 1221 cost= 0.483728499\n",
      "Epoch: 1231 cost= 0.480985935\n",
      "Epoch: 1241 cost= 0.482538892\n",
      "Epoch: 1251 cost= 0.478252981\n",
      "Epoch: 1261 cost= 0.481298607\n",
      "Epoch: 1271 cost= 0.482378199\n",
      "Epoch: 1281 cost= 0.480185497\n",
      "Epoch: 1291 cost= 0.482466015\n",
      "Epoch: 1301 cost= 0.479877300\n",
      "Epoch: 1311 cost= 0.479001142\n",
      "Epoch: 1321 cost= 0.480540246\n",
      "Epoch: 1331 cost= 0.480232340\n",
      "Epoch: 1341 cost= 0.476779589\n",
      "Epoch: 1351 cost= 0.479002048\n",
      "Epoch: 1361 cost= 0.478205089\n",
      "Epoch: 1371 cost= 0.481538800\n",
      "Epoch: 1381 cost= 0.480249650\n",
      "Epoch: 1391 cost= 0.477374374\n",
      "Epoch: 1401 cost= 0.481299017\n",
      "Epoch: 1411 cost= 0.480137945\n",
      "Epoch: 1421 cost= 0.478403869\n",
      "Epoch: 1431 cost= 0.478434034\n",
      "Epoch: 1441 cost= 0.479660396\n",
      "Epoch: 1451 cost= 0.479497073\n",
      "Epoch: 1461 cost= 0.479979285\n",
      "Epoch: 1471 cost= 0.476805722\n",
      "Epoch: 1481 cost= 0.481835123\n",
      "Epoch: 1491 cost= 0.480236403\n",
      "Epoch: 1501 cost= 0.479141987\n",
      "Epoch: 1511 cost= 0.478060408\n",
      "Epoch: 1521 cost= 0.481307949\n",
      "Epoch: 1531 cost= 0.481029909\n",
      "Epoch: 1541 cost= 0.480962262\n",
      "Epoch: 1551 cost= 0.480772539\n",
      "Epoch: 1561 cost= 0.475787289\n",
      "Epoch: 1571 cost= 0.477027200\n",
      "Epoch: 1581 cost= 0.475917264\n",
      "Epoch: 1591 cost= 0.479391764\n",
      "Epoch: 1601 cost= 0.479251668\n",
      "Epoch: 1611 cost= 0.477434861\n",
      "Epoch: 1621 cost= 0.479555740\n",
      "Epoch: 1631 cost= 0.479424367\n",
      "Epoch: 1641 cost= 0.480083721\n",
      "Epoch: 1651 cost= 0.477423839\n",
      "Epoch: 1661 cost= 0.477650421\n",
      "Epoch: 1671 cost= 0.481406768\n",
      "Epoch: 1681 cost= 0.475022299\n",
      "Epoch: 1691 cost= 0.476646839\n",
      "Epoch: 1701 cost= 0.476139969\n",
      "Epoch: 1711 cost= 0.479616055\n",
      "Epoch: 1721 cost= 0.479331984\n",
      "Epoch: 1731 cost= 0.477680043\n",
      "Epoch: 1741 cost= 0.478823163\n",
      "Epoch: 1751 cost= 0.479480218\n",
      "Epoch: 1761 cost= 0.477210511\n",
      "Epoch: 1771 cost= 0.478284086\n",
      "Epoch: 1781 cost= 0.478083449\n",
      "Epoch: 1791 cost= 0.477849377\n",
      "Epoch: 1801 cost= 0.478341478\n",
      "Epoch: 1811 cost= 0.480147056\n",
      "Epoch: 1821 cost= 0.478599344\n",
      "Epoch: 1831 cost= 0.475116901\n",
      "Epoch: 1841 cost= 0.478758146\n",
      "Epoch: 1851 cost= 0.477055540\n",
      "Epoch: 1861 cost= 0.476049774\n",
      "Epoch: 1871 cost= 0.476892150\n",
      "Epoch: 1881 cost= 0.477406123\n",
      "Epoch: 1891 cost= 0.478962956\n",
      "Epoch: 1901 cost= 0.479522644\n",
      "Epoch: 1911 cost= 0.475326247\n",
      "Epoch: 1921 cost= 0.477286325\n",
      "Epoch: 1931 cost= 0.474782119\n",
      "Epoch: 1941 cost= 0.476274763\n",
      "Epoch: 1951 cost= 0.474468130\n",
      "Epoch: 1961 cost= 0.476369971\n",
      "Epoch: 1971 cost= 0.477226456\n",
      "Epoch: 1981 cost= 0.476881176\n",
      "Epoch: 1991 cost= 0.481041614\n",
      "Optimization Finished!\n",
      "Accuracy: 0.8238156\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.0\n",
    "        total_batch = int(len(train_x) / batch_size)\n",
    "        x_batches = np.array_split(train_x, total_batch)\n",
    "        y_batches = np.array_split(train_y, total_batch)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = x_batches[i], y_batches[i]\n",
    "            _, c = sess.run([optimizer, cost], \n",
    "                            feed_dict={\n",
    "                                x: batch_x, \n",
    "                                y: batch_y, \n",
    "                                keep_prob: 0.8\n",
    "                            })\n",
    "            avg_cost += c / total_batch\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "    correct_prediction = tf.equal(tf.argmax(predictions, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: test_x, y: test_y, keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(\"Accuracy:\", accuracy.eval({x: test_x, y: test_y, keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
